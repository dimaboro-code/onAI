# Описание проекта

Данный проект выступает в качестве клиента для LLM-моделей (например, GPT-3.5-turbo, Claude, LLaMA и т.д.) через API, поддерживает приём Webhook-запросов и отправку результатов на указанный callback URL, а также сохраняет историю сообщений для поддержания контекста диалога.

## Функциональные особенности

1. **Вебхук-эндпоинт (POST /webhook)**  
   Принимает входящие запросы и запускает асинхронную обработку.

2. **Интеграция LLM-моделей**  
   Используются различные модели через сервис openrouter (бесплатная демо-версия). Запросы формируются в формате, совместимом с OpenAI API.

3. **Отправка результатов**  
   Проект отправляет результаты обработки на указанный callback URL.

4. **История сообщений**  
   Поддерживается контекст диалога за счёт хранения истории сообщений.

## Технические детали

- **Стек**: Python, FastAPI, RabbitMQ, Redis, PostgreSQL, Docker.  
- **Библиотеки**:  
  - SQLAlchemy   
  - openai  
  - fastapi-limiter  
  - pytest  

### Основные блоки

1. **FastAPI** — для REST API (эндпоинты, валидация, логирование).  
2. **RabbitMQ** — очередь, используется для масштабирования:  
   - Producer (отправляет задачи на выполнение)  
   - Consumer (обрабатывает полученные задачи)  
3. **Redis** — используется для кэширования/ограничения запросов.  
4. **PostgreSQL** — хранение истории сообщений, запросов, результатов.  
5. **Тестирование**: Pytest + coverage (покрытие 94 %).

### Запуск проекта

1. Склонируйте репозиторий:  
   git clone <ссылка-на-репозиторий>

2. Создайте .env-файл (при необходимости) и настройте переменные окружения (RabbitMQ, Redis, Postgres и т.д.).

3. Запустите проект командой:  
   docker-compose up --build

4. Проект поднимется вместе со всеми сервисами (RabbitMQ, Redis, PostgreSQL). После успешного запуска можно обращаться к эндпоинтам по адресу:  
   http://localhost:8080

### Настройка ограничений (rate-limiter)

- Используется fastapi-limiter. Лимиты можно настроить в конфиге (например, X запросов в минуту).  

### Тесты

- Для запуска тестов внутри контейнера:  
  docker-compose run --rm consumer pytest --cov=src --cov=tests

- Результат покрытия тестами (coverage report):
```

  Name                     Stmts   Miss  Cover  
  --------------------------------------------  
  src/__init__.py              0      0   100%  
  src/config.py               27      0   100%  
  src/consumer.py             53      8    85%  
  src/database.py             69     15    78%  
  src/models.py                6      0   100%  
  src/openai_service.py       14      0   100%  
  src/producer.py             53      9    83%  
  src/rabbit.py               37      0   100%  
  tests/__init__.py            0      0   100%  
  tests/test_consumer.py      75      0   100%  
  tests/test_db.py            79      0   100%  
  tests/test_openai.py        20      0   100%  
  tests/test_producer.py      48      0   100%  
  tests/test_rabbit.py        83      0   100%  
  --------------------------------------------  
  TOTAL                      564     32    94%  
```
## Дополнительно

1. **Асинхронная обработка**  
   На уровне FastAPI (async/await), а также асинхронная интеграция с RabbitMQ.  
2. **Логирование**  
   Записи о запросах, ошибках и результатах сохраняются в логе.  
3. **Обработка ошибок**  
   Предусмотрена базовая обработка исключений и возврат корректных HTTP-статусов.  
4. **Документация API (Swagger/OpenAPI)**  
   Генерируется автоматически FastAPI (http://localhost:8080/docs).

При возникновении вопросов или проблем открывайте Issues в репозитории или обращайтесь к автору напрямую. Спасибо за использование!